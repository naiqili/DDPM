{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 10:47:48.636425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os, time, datetime\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm, trange\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "\n",
    "from nice import NICE\n",
    "from dircluster import sample_mu_lam, SSE, LLH\n",
    "from utils import savefig_clusters, mvnlogpdf, plt2img\n",
    "from AE import AE\n",
    "\n",
    "import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import log, exp, pi\n",
    "from scipy.stats import wishart, gamma\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "from numpy.linalg import inv, det\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.special import loggamma\n",
    "import tensorflow as tf\n",
    "from scipy.stats import wishart, gamma, entropy\n",
    "# from plot_cf import pretty_plot_confusion_matrix\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "from mymetrics import v_measure_score, adjusted_rand_score, pair_confusion_matrix, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "# matplotlib.use('Agg')\n",
    "from ae_util import *\n",
    "from utils import *\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import datasets\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'  # using specific GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=123456\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iter_num(n_epoch, n_samples, cfg_list):\n",
    "    '''\n",
    "    How many steps for iteration in the n-th epoch\n",
    "    num_iter is configed to decay for fast training \n",
    "    '''\n",
    "    ratio = cfg_list[-1] if n_epoch >= len(cfg_list) else cfg_list[n_epoch]\n",
    "    return int(np.ceil(n_samples * ratio))\n",
    "\n",
    "\n",
    "def cluster2label(K, samples_k, y):\n",
    "    map_dict = {}\n",
    "    y_pred = samples_k.copy()\n",
    "    for k_idx in range(K):  # analysis each cluster\n",
    "        cluster_sid = (samples_k == k_idx)\n",
    "        label, counts = np.unique(y[cluster_sid], return_counts=True)\n",
    "        major_class_idx = np.argmax(counts)\n",
    "\n",
    "        map_dict[k_idx] = label[major_class_idx]\n",
    "        y_pred[cluster_sid] = label[major_class_idx]\n",
    "    return y_pred, map_dict\n",
    "\n",
    "\n",
    "def pair_f1_score(ys, ks):\n",
    "    cf = pair_confusion_matrix(ys, ks)\n",
    "    # TN, FP\n",
    "    # FN, TP\n",
    "    return cf[1, 1] / float(cf[1, 1] + 0.5 * (cf[0, 1] + cf[1, 0]))\n",
    "\n",
    "\n",
    "def get_count_matrix(K, cluster_label, nC, label):\n",
    "    # count labels in each cluster\n",
    "    np.unique(cluster_label)\n",
    "    cluster_counts_list = []\n",
    "    for k in range(K):\n",
    "        c_sample_id = (cluster_label == k)\n",
    "        y, ny = np.unique(label[c_sample_id], return_counts=True)\n",
    "        counts_arr = np.zeros(nC)\n",
    "        counts_arr[y] = ny\n",
    "        cluster_counts_list.append(counts_arr)\n",
    "    count_matrix = np.stack(cluster_counts_list, axis=0)\n",
    "    return count_matrix\n",
    "\n",
    "\n",
    "def cluster_acc_fixed(Y_pred, Y):  # from paper VaDE code\n",
    "    from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "    assert Y_pred.size == Y.size\n",
    "    D = max(Y_pred.max(), Y.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(Y_pred.size):\n",
    "        w[Y_pred[i], Y[i]] += 1\n",
    "    a, b = linear_assignment(w.max() - w)\n",
    "    ind = [(a[i], b[i]) for i in range(a.shape[0])]\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / Y_pred.size, ind\n",
    "\n",
    "\n",
    "def eval_cluster(samples_k, ys, return_extra=False, alg='default', ds_name='default'):\n",
    "    cids, nK = np.unique(samples_k, return_counts=True)\n",
    "    k_indices = np.argsort(-nK)  # desc sort by size\n",
    "    k_major, k_entropy = [], []\n",
    "    confusion_matrix = np.zeros(shape=(10, 10), dtype=float)\n",
    "    correct = 0\n",
    "    for k_idx in k_indices:  # analysis each cluster\n",
    "        cluster_sid = (samples_k == cids[k_idx])\n",
    "        cluster_size = nK[k_idx]\n",
    "        label, counts = np.unique(ys[cluster_sid], return_counts=True)\n",
    "        k_entropy.append(entropy(counts / cluster_size))\n",
    "\n",
    "        major_class_idx = np.argmax(counts)\n",
    "        k_major.append(label[major_class_idx])\n",
    "        confusion_matrix[label, label[major_class_idx]] += counts\n",
    "        correct += counts[major_class_idx]\n",
    "    acc = correct / np.sum(nK)\n",
    "    weighted_entropy = np.sum(nK[k_indices]/np.sum(nK) * k_entropy)\n",
    "\n",
    "    # k-irrevlent measure\n",
    "    v_score, adj_rand = v_measure_score(ys, samples_k), adjusted_rand_score(ys, samples_k),\n",
    "    pair_f1 = pair_f1_score(ys, samples_k)\n",
    "    if not return_extra: \n",
    "        return acc, v_score, adj_rand, pair_f1, weighted_entropy, confusion_matrix\n",
    "    else:\n",
    "        nmi = normalized_mutual_info_score(ys, samples_k) \n",
    "        return pair_f1, v_score, adj_rand, nmi, acc, len(cids), weighted_entropy, confusion_matrix\n",
    "\n",
    "\n",
    "def dirichlet_clustering(global_epoch, tb_writter, dir_params, samples, ys, n_iter_samples, args, log_step=0):\n",
    "    hp = dir_params.hyper\n",
    "    _mu0, _ka0, logalpha, _a0, _b0 = hp.mu0, hp.ka0, hp.logalpha, hp.a0, hp.b0\n",
    "    K, lam_K, mu_K, nK, ks = dir_params.K, dir_params.lam_K, dir_params.mu_K, dir_params.n_K, dir_params.samples_k\n",
    "\n",
    "    # begin iter\n",
    "    monitor_freq = 100\n",
    "    with tb_writter.as_default():\n",
    "        for iter_idx in range(n_iter_samples):\n",
    "            ## report result\n",
    "            global_log_step = iter_idx+log_step\n",
    "            sample_idx = global_log_step % len(ks)  # mod by n_sample_load\n",
    "\n",
    "            # others\n",
    "            if (global_log_step + 1) % monitor_freq == 0:\n",
    "                # report cluster information\n",
    "                tf.summary.scalar('dmm/K', K, step=iter_idx+log_step)\n",
    "                arr = np.asarray(nK)/sum(nK)\n",
    "                tf.summary.histogram('cluster size ratio', arr, step=global_log_step)\n",
    "                mu_norm = np.linalg.norm(np.asarray(mu_K), axis=1)\n",
    "                tf.summary.histogram('cluster mu norm', mu_norm, step=global_log_step)\n",
    "                tf.summary.histogram('cluster lambda', np.asarray(lam_K), step=global_log_step)\n",
    "\n",
    "                # report metrics\n",
    "#                 if (global_log_step + 1) % (monitor_freq*10) == 0:\n",
    "                error = SSE(samples, K, ks, nK, mu_K, lam_K)\n",
    "                pair_f1, v_score, adj_rand, nmi, acc, _, w_ent, _ = eval_cluster(ks, ys,return_extra=True, ds_name=args.dataset)\n",
    "                tf.summary.scalar('dmm/sse', error, step=global_log_step)\n",
    "                tf.summary.scalar('dmm/acc', acc, step=global_log_step)\n",
    "                tf.summary.scalar('dmm/v_score', v_score, step=global_log_step)\n",
    "                tf.summary.scalar('dmm/nmi', nmi, step=global_log_step)\n",
    "                tf.summary.scalar('dmm/adj_rand', adj_rand, step=global_log_step)\n",
    "                tf.summary.scalar('dmm/pair_f1', pair_f1, step=global_log_step)\n",
    "                tf.summary.scalar('dmm/w_entropy', w_ent, step=global_log_step)\n",
    "                tb_writter.flush()\n",
    "                \n",
    "            xi = samples[sample_idx]\n",
    "            old_k = ks[sample_idx]\n",
    "            if old_k != -1: \n",
    "                nK[old_k] -= 1\n",
    "                if nK[old_k] == 0: \n",
    "                    idx = (ks == K - 1)\n",
    "                    ks[idx] = old_k\n",
    "                    nK[old_k], lam_K[old_k], mu_K[old_k], = nK[K - 1], lam_K[K - 1], mu_K[K - 1]\n",
    "                    nK, lam_K, mu_K = nK[:-1], lam_K[:-1], mu_K[:-1]\n",
    "                    K -= 1\n",
    "\n",
    "            p_lst = []  # p_lst[k] is the probability that xi in C_k, p_lst[-1] means xi forms a new cluster\n",
    "            Kn = K \n",
    "            if K == 0:  # if no cluster, just build a new one\n",
    "                chosen_k = 0\n",
    "            else:\n",
    "                # existing cluster assignment prob\n",
    "                Klst = np.random.choice(K, size=Kn, replace=False, p=np.asarray(nK) / np.sum(np.asarray(nK)))\n",
    "                for k in Klst:\n",
    "                    pk = log(nK[k]) + mvnlogpdf(xi, mu_K[k], 1 / lam_K[k])\n",
    "                    p_lst.append(pk)\n",
    "\n",
    "                # new cluster assignment pob.\n",
    "                _kan = _ka0 + 1\n",
    "                _an = _a0 + 0.5 * args.dim\n",
    "                _bn = _b0 + _ka0 * np.linalg.norm(xi - _mu0) ** 2 / (2 * (_ka0 + 1))\n",
    "                logpk = log(logalpha) + \\\n",
    "                        loggamma(_an) - loggamma(_a0) + \\\n",
    "                        _a0 * log(_b0) - _an * log(_bn) + \\\n",
    "                        0.5 * (log(_ka0) - log(_kan)) - args.dim / 2 * log(2 * pi)\n",
    "                p_lst.append(logpk)\n",
    "\n",
    "                # sampling according to the assignment prob.\n",
    "                maxpk = max(p_lst)\n",
    "                p_lst = [exp(v - maxpk) for v in p_lst]  \n",
    "                chosen_k = np.random.choice(list(range(Kn + 1)),\n",
    "                                            p=p_lst / sum(p_lst))  # sample, now xi belongs cluster chosen_k\n",
    "\n",
    "            # 3.1.2 Sampling Cluster mean and lambda\n",
    "            if chosen_k == Kn:  # assigned to new cluster\n",
    "                nK.append(1)\n",
    "                ks[sample_idx] = K\n",
    "                lam_k, mu_k = sample_mu_lam(samples, nK, ks, chosen_k, _mu0, _ka0, _a0, _b0)\n",
    "                mu_K.append(mu_k)\n",
    "                lam_K.append(lam_k)\n",
    "                K += 1\n",
    "            else:  # assigned to existing cluster\n",
    "                chosen_k = Klst[chosen_k]\n",
    "                nK[chosen_k] += 1\n",
    "                ks[sample_idx] = chosen_k\n",
    "                \n",
    "            if sample_idx % args.dmm_rebuild_freq == 0 or sample_idx == args.n_sample_load:\n",
    "                for k in range(K):\n",
    "                    lam_K[k], mu_K[k] = sample_mu_lam(samples, nK, ks, k, _mu0, _ka0, _a0, _b0)\n",
    "\n",
    "        # the last iter\n",
    "        pass\n",
    "        tf.summary.scalar('dmm_epoch/sse', error, step=global_epoch)\n",
    "        tf.summary.scalar('dmm_epoch/acc', acc, step=global_epoch)\n",
    "        tf.summary.scalar('dmm_epoch/v_score', v_score, step=global_epoch)\n",
    "        tf.summary.scalar('dmm_epoch/adj_rand', adj_rand, step=global_epoch)\n",
    "        tf.summary.scalar('dmm_epoch/pair_f1', pair_f1, step=global_epoch)\n",
    "        tf.summary.scalar('dmm_epoch/nmi', nmi, step=global_epoch)\n",
    "        tf.summary.scalar('dmm_epoch/w_entropy', w_ent, step=global_epoch)\n",
    "    # update params\n",
    "    dir_params.K, dir_params.lam_K, dir_params.mu_K, dir_params.n_K, dir_params.samples_k = K, lam_K, mu_K, nK, ks\n",
    "    return dir_params\n",
    "\n",
    "def train_flow(global_epoch, tb_writter, model_nice, opt, ds_inf_iter, n_iter, dir_params, args,\n",
    "               log_step=0, prev_z_repr=None):\n",
    "    if n_iter == 0:\n",
    "        print('flow_opt=0, train ignored')\n",
    "        return\n",
    "    model_nice = model_nice.to(args.device)\n",
    "    epoch_n_batch = args.n_sample_load // TRAIN_BATCH_SIZE\n",
    "    epoch_loss, epoch = 0.0, 0\n",
    "    K, ks, ns, mu_K, lam_K = dir_params.K, dir_params.samples_k, dir_params.n_K, dir_params.mu_K, dir_params.lam_K\n",
    "\n",
    "    model_nice.to(args.device)\n",
    "    with tb_writter.as_default():\n",
    "        for n_batch in range(n_iter):\n",
    "            (_, x, _, idx) = next(ds_inf_iter)\n",
    "            x = x.to(args.device)\n",
    "\n",
    "            model_nice.train()\n",
    "            opt.zero_grad()\n",
    "            x = x.to(args.device)\n",
    "            _, likelihood = model_nice(x, K, ks[idx.numpy()], ns, np.asarray(mu_K), np.asarray(lam_K))\n",
    "            if isinstance(likelihood, int):\n",
    "                continue\n",
    "            loss = -torch.mean(likelihood)  # NLL\n",
    "            \n",
    "            loss.backward() \n",
    "            opt.step() \n",
    "\n",
    "\n",
    "            tf.summary.scalar('flow/batch_nll_loss', loss.detach().cpu().numpy(), step=n_batch + global_epoch * n_iter)\n",
    "            if (n_batch + 1) % epoch_n_batch == 0:\n",
    "                tb_writter.flush()\n",
    "                tf.summary.scalar('flow/epoch_nll_loss', epoch_loss / epoch_n_batch,\n",
    "                                  step=int(epoch + global_epoch * (n_iter / epoch_n_batch)))\n",
    "                epoch_loss *= 0.0\n",
    "                epoch += 1\n",
    "            else:\n",
    "                epoch_loss += loss.detach().cpu().numpy() \n",
    "\n",
    "\n",
    "def init_dir_params(args):\n",
    "    dir_params = Namespace(\n",
    "        hyper=Namespace(\n",
    "            a0=args.a0 * args.dim, b0=args.b0 * args.dim,\n",
    "            mu0=np.zeros(args.dim), ka0=args.kappa0,\n",
    "            logalpha=args.logalpha\n",
    "        ),\n",
    "        K=0, n_K=[], lam_K=[], mu_K=[], samples_k=np.ones(args.n_sample_load, dtype=int) * -1\n",
    "        # K: total num of clusters. lam_K, mu_K: sampled mu/lambda cluster. n_K: size of clusters\n",
    "        # samples_k: cluster index for each sample, -1 means unassigned\n",
    "    )\n",
    "    return dir_params\n",
    "\n",
    "\n",
    "def init_flow_model(args):\n",
    "    model_nice = NICE(data_dim=args.dim, num_coupling_layers=args.nice_nlayers,\n",
    "                      num_hidden_units=args.nice_units, device_name=args.device)\n",
    "#     opt = optim.SGD(model_nice.parameters(), args.lr)\n",
    "    opt = optim.Adam(model_nice.parameters(), args.lr)\n",
    "    return model_nice, opt\n",
    "\n",
    "\n",
    "def load_ae_dataset(ds_name, n_sample_load, aex_file, idx_load=None):\n",
    "    print('start loading ae dataset')\n",
    "    if ds_name == 'mnist':\n",
    "        ds_raw = load_dataset(ds_name, './data', split='train')\n",
    "        if idx_load is None:\n",
    "            ds_ae_repr = torch.load(aex_file)[:n_sample_load]\n",
    "        else:\n",
    "            ds_ae_repr = torch.load(aex_file)[idx_load]\n",
    "        ds_ae = AEDataset(ds_raw, ds_ae_repr, n_sample_load, idx_load=idx_load)\n",
    "    elif 'VaDE_' in ds_name:\n",
    "        X, Y, Z = np.load(f'../VaDE/{ds_name}_full_X.npz')['arr_0'], \\\n",
    "                  np.load(f'../VaDE/{ds_name}_full_Y.npz')['arr_0'].reshape(-1, ), \\\n",
    "                  np.load(f'../VaDE/{ds_name}_full_Z.npz')['arr_0']\n",
    "        # X = (X - X.mean()) / X.std()  # todo critical norm\n",
    "        if idx_load is None:\n",
    "            ds_ae_repr = torch.from_numpy(Z)[:n_sample_load]\n",
    "        else:\n",
    "            ds_ae_repr = torch.from_numpy(Z)[idx_load]\n",
    "        ds_ae = OtherAEDataset(X, Y, Z, n_sample_load, idx_load=idx_load)\n",
    "    else:\n",
    "        raise ValueError(f'dataset name {ds_name} not supported')\n",
    "    return ds_ae_repr, ds_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TmpArgs:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'VaDE_stl'\n",
    "        self.dim = 10\n",
    "        self.noise = 0.0 \n",
    "        self.n_sample_full = self.n_sample_load = 13000\n",
    "        self.epoch = 5   #30\n",
    "        self.dir_epoch = 3  #10\n",
    "        self.iter_dmm = self.n_sample_load*3\n",
    "        self.dmm_rebuild_freq = 50 # self.n_sample_load // 50\n",
    "        self.iter_nice = self.n_sample_load // 5\n",
    "        self.save_freq = 1\n",
    "        self.lr = 0.000001\n",
    "        self.logalpha = 1e-10\n",
    "        self.kappa0 = 0.005\n",
    "        self.a0 = 1000\n",
    "        self.b0 = 100\n",
    "        self.nice_nlayers = 6\n",
    "        self.nice_units = 512\n",
    "        self.exp_name = 'stl'\n",
    "        self.pretrainpath = None\n",
    "        self.aex_file = None\n",
    "        self.device = 'cuda'\n",
    "        self.log_dir = './exp_out/stl'\n",
    "        \n",
    "args = TmpArgs()    #parse_args(manual_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading ae dataset\n",
      "transfrom consumed 22.2s\n",
      "data loaded, cls dist: N=13000[[   0    1    2    3    4    5    6    7    8    9]\n",
      " [1300 1300 1300 1300 1300 1300 1300 1300 1300 1300]]\n",
      "14565-DDPM training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 10:48:14.833461: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-12 10:48:14.834067: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-02-12 10:48:14.834815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:84:00.0 name: Tesla M60 computeCapability: 5.2\n",
      "coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s\n",
      "2022-02-12 10:48:14.834835: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-12 10:48:14.834872: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-12 10:48:14.834892: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-02-12 10:48:14.834911: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-02-12 10:48:14.834930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-02-12 10:48:14.834953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-02-12 10:48:14.834972: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-02-12 10:48:14.837774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-12 10:48:14.838874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-02-12 10:48:14.839210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-12 10:48:14.841040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:84:00.0 name: Tesla M60 computeCapability: 5.2\n",
      "coreClock: 1.1775GHz coreCount: 16 deviceMemorySize: 7.94GiB deviceMemoryBandwidth: 149.31GiB/s\n",
      "2022-02-12 10:48:14.841066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-12 10:48:14.841078: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-12 10:48:14.841087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-02-12 10:48:14.841096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-02-12 10:48:14.841104: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-02-12 10:48:14.841114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-02-12 10:48:14.841123: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-02-12 10:48:14.841142: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-12 10:48:14.842174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-02-12 10:48:14.842283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-02-12 10:48:14.842293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-02-12 10:48:14.842298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-02-12 10:48:14.843982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7057 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:84:00.0, compute capability: 5.2)\n",
      "2022-02-12 10:48:14.844193: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14565-DDPM training epoch 1\n",
      "14565-DDPM training epoch 2\n"
     ]
    }
   ],
   "source": [
    "time_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "exp_id = 'dir_%s_%s_N%d_E%d_kappa0_%s_a0_%s_b0_%s_alpha_%s_%s_p%s' % (\n",
    "        args.exp_name, args.dataset, args.n_sample_load, args.epoch, args.kappa0, args.a0, args.b0, args.logalpha, time_str, os.getpid())\n",
    "task_dir = os.path.join(args.log_dir, exp_id)\n",
    "if not os.path.exists(task_dir):\n",
    "    os.makedirs(task_dir)\n",
    "\n",
    "model_nice, opt = init_flow_model(args)\n",
    "dir_params = init_dir_params(args)\n",
    "ds_ae_repr, ds_ae = load_ae_dataset(args.dataset, args.n_sample_load, args.aex_file)\n",
    "samples_flow_z = transform_z(model_nice, ds_ae, args.n_sample_load)\n",
    "dl_ae = InfiniteDataLoader(dataset=ds_ae, batch_size=TRAIN_BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "iter_ae_inf = iter(dl_ae)\n",
    "print(f'data loaded, cls dist: N={args.n_sample_load}'\n",
    "      f'{np.stack(np.unique(ds_ae.targets, return_counts=True))}')\n",
    "\n",
    "tb_logger = tf.summary.create_file_writer(task_dir)\n",
    "\n",
    "global_step_dmm, global_step_flow = 0, 0\n",
    "\n",
    "# samples_flow_z = transform_z(model_nice, ds_ae, args.n_sample_load)\n",
    "for n_epoch in range(args.dir_epoch):\n",
    "    print(f'{os.getpid()}-DDPM training epoch {n_epoch}')\n",
    "    n_iter_clst = args.iter_dmm\n",
    "    dir_params = dirichlet_clustering(n_epoch, tb_logger, dir_params, samples_flow_z, ds_ae.targets, n_iter_clst, args, log_step=global_step_dmm)\n",
    "    global_step_dmm += n_iter_clst+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14565-DDPM training epoch 0\n",
      "transfrom consumed 22.9s\n",
      "14565-DDPM training epoch 1\n",
      "transfrom consumed 23.1s\n",
      "14565-DDPM training epoch 2\n",
      "transfrom consumed 23.0s\n",
      "14565-DDPM training epoch 3\n",
      "transfrom consumed 30.2s\n",
      "14565-DDPM training epoch 4\n",
      "transfrom consumed 23.1s\n"
     ]
    }
   ],
   "source": [
    "exp_id = 'full_%s_%s_N%d_E%d_kappa0_%s_a0_%s_b0_%s_alpha_%s_%s_p%s' % (\n",
    "        args.exp_name, args.dataset, args.n_sample_load, args.epoch, args.kappa0, args.a0, args.b0, args.logalpha, time_str, os.getpid())\n",
    "task_dir = os.path.join(args.log_dir, exp_id)\n",
    "if not os.path.exists(task_dir):\n",
    "    os.makedirs(task_dir)\n",
    "\n",
    "model_nice, opt = init_flow_model(args)\n",
    "# ds_ae_repr, ds_ae = load_ae_dataset(args.dataset, args.n_sample_load, args.aex_file)\n",
    "# samples_flow_z = transform_z(model_nice, ds_ae, args.n_sample_load)\n",
    "# 4.alternated training\n",
    "tb_logger = tf.summary.create_file_writer(task_dir)\n",
    "\n",
    "global_step_dmm, global_step_flow = 0, 0\n",
    "\n",
    "for n_epoch in range(args.epoch):\n",
    "    print(f'{os.getpid()}-DDPM training epoch {n_epoch}') \n",
    "    samples_flow_z = transform_z(model_nice, ds_ae, args.n_sample_load) \n",
    "    n_iter_clst = args.iter_dmm\n",
    "    dir_params = dirichlet_clustering(n_epoch, tb_logger, dir_params, samples_flow_z, ds_ae.targets, n_iter_clst, args, log_step=global_step_dmm)\n",
    "    global_step_dmm += n_iter_clst+1\n",
    "    \n",
    "    n_iter_opt = args.iter_nice\n",
    "    train_flow(n_epoch, tb_logger, model_nice, opt, iter_ae_inf, n_iter_opt, dir_params, args,\n",
    "               log_step=global_step_flow, prev_z_repr=samples_flow_z)\n",
    "    global_step_flow += n_iter_opt+1\n",
    "\n",
    "    # save params\n",
    "    if (n_epoch+1) % args.save_freq == 0 or n_epoch == args.epoch-1:\n",
    "        # save flow model\n",
    "        torch.save(model_nice.state_dict(), os.path.join(task_dir, f'flow_E{n_epoch}.pt'))\n",
    "\n",
    "        # save dpm params\n",
    "        np.savez(os.path.join(task_dir, f'dpm_E{n_epoch}'), K=dir_params.K, ks=dir_params.samples_k, ns=dir_params.n_K,\n",
    "                 mu_K=np.asarray(dir_params.mu_K), lam_K=np.asarray(dir_params.lam_K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-py37",
   "language": "python",
   "name": "jack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
